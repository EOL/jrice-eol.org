x-app: &default-app
  image: publishing
  restart: always
  environment: # NOTE: see https://docs.docker.com/compose/environment-variables/ for details.
    - RAILS_ENV
    - RAILS_MASTER_KEY
    - ELASTICSEARCH_URL
    - REDIS_HOST
    - NODE_OPTIONS=--openssl-legacy-provider npm run start
  depends_on:
    - mysql
    - neo4j
    - memcached
    - redis
    - elasticsearch
  deploy:
    resources:
      limits:
        cpus: 2
        # This seems REALLY high, but, yeah, this is how much it needs. Alas.
        memory: 8G
    restart_policy:
      condition: on-failure
      delay: 16s
      max_attempts: 3
      window: 240s
  profiles: ["app"]
  logging:
    driver: local
    options:
      max-file: 3
      max-size: 5m

x-worker: &default-worker
  image: publishing
  restart: always
  environment: # NOTE: see https://docs.docker.com/compose/environment-variables/ for details.
    - RAILS_ENV
    - RAILS_MASTER_KEY
    - ELASTICSEARCH_URL
    - REDIS_HOST
    - NODE_OPTIONS=--openssl-legacy-provider npm run start
  depends_on:
    - mysql
    - neo4j
    - memcached
    - redis
    - elasticsearch
  deploy:
    resources:
      limits:
        cpus: '0.5'
        memory: 8G
    restart_policy:
      condition: on-failure
      delay: 8s
      max_attempts: 3
      window: 120s
  profiles: ["worker"]
  logging:
    driver: "local"
    options:
      max-file: "3"
      max-size: "5m"

x-default-deploy: &default-deploy
  deploy:
    resources:
      limits:
        cpus: '0.5'
        memory: 1G
    restart_policy:
      condition: on-failure
      delay: 8s
      max_attempts: 3
      window: 120s
  logging:
    driver: "local"
    options:
      max-file: "3"
      max-size: "5m"

services:
  elasticsearch:
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: '4.5G'
      restart_policy:
        condition: on-failure
        delay: 8s
        max_attempts: 3
        window: 120s
    image: docker.elastic.co/elasticsearch/elasticsearch:6.8.9
    container_name: publishing_elasticsearch
    restart: always
    environment:
      - bootstrap.memory_lock=true
      - cluster.name=eol-search-${RAILS_ENV}
      - "ES_JAVA_OPTS=-Xms4g -Xmx4g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ./templates/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - /data/publishing_elasticsearch/data:/var/data/elasticsearch
      - /data/publishing_elasticsearch/log:/var/log/elasticsearch
    ports:
      - 9200:9200
    profiles: ["search"]
    logging:
      driver: "local"
      options:
        max-file: "3"
        max-size: "5m"
  memcached:
    <<: *default-deploy
    image: memcached
    container_name: publishing_memcached
    restart: always
    environment:
      - TZ=America/New_York
    command: memcached -m 4096m
    profiles: ["cache"]
  neo4j:
    <<: *default-deploy
    profiles: ["graph"]
    image: neo4j:4.2.3
    container_name: neo4j
    restart: always
    # NOTE: environment variables OVERRIDE the values in the config file!
    # Results are then applied to an artificial copy of the config in
    # $HOME/conf/neo4j.conf, which can be VERY confusing. Be aware.
    environment:
      - TZ=America/New_York
      - NEO4J_AUTH=neo4j/SomePasswordHere
      - NEO4J_dbms_memory_pagecache_size=34G
      - NEO4J_dbms_memory_heap_max__size=31500m
      - NEO4J_dbms_memory_heap_initial__size=31500m
      - NEO4J_cypher_query__max__allocations_size=4G
      - NEO4J_dbms_transaction_timeout=85s
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_directories_import=/exports
      - NEO4J_apoc_export_file_enabled=true
    volumes:
      - /data/neo4j/data:/data
      - /data/neo4j/logs:/logs
      - /data/neo4j/plugins:/plugins
      - /data/neo4j/exports:/exports
      - ./templates/neo4j.conf:/conf/neo4j.conf
    ports:
      - 7473:7473
      - 7474:7474
      - 7687:7687
  mysql:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 16G
      restart_policy:
        condition: on-failure
        delay: 8s
        max_attempts: 3
        window: 120s
    logging:
      driver: "local"
      options:
        max-file: "3"
        max-size: "5m"
    image: mysql:8
    container_name: eol_publishing_mysql_${RAILS_ENV}
    restart: always
    environment:
      - TZ=America/New_York
      - MYSQL_ROOT_PASSWORD=SomethingReallySecure
      - MYSQL_DATABASE=eol_web_ENV
      - MYSQL_USER=admin
      - MYSQL_PASSWORD=SomethingSuperSecureHereToo
    profiles: ["sql"]
    volumes:
      - /u/data/eol_publishing_mysql_${RAILS_ENV}:/var/lib/mysql
      - /u/data/eol_publishing_mysql_${RAILS_ENV}_temp:/tmp
      - /u/data/eol_publishing_mysql_${RAILS_ENV}_conf:/etc/mysql/conf.d/
    ports:
      - 3306:3306
  redis: # WARNING: It's simpler to just skip persistence, but that means we lose the entire queue on restart.
    <<: *default-deploy
    profiles: ["redis"]
    image: redis:6.0
    container_name: redis
    restart: always
    ports:
      - 6379:6379
  app:
    <<: *default-app
    build:
      context: ..
    command: bundle exec rails s -p 3000 -b '0.0.0.0'
    volumes:
      - /data/publishing_web:/app/public/data
      - /data/publishing_web_log:/app/log
      - /data/publishing_web_private:/app/data
    ports:
      - "3000"
    deploy:
      replicas: 6
  pub_sidekiq:
    <<: *default-worker
    container_name: pub_sidekiq
    command: bundle exec sidekiq
    volumes:
      - /data/publishing_web:/app/public/data
      - /data/publishing_sidekiq_log:/app/log
      - /data/publishing_web_private:/app/data
  pub_crono:
    <<: *default-worker
    container_name: pub_crono
    command: bundle exec crono
    volumes: # NOTE: YES, it shares the data volumes with app, so it can serve the files that were processed here.
      - /data/publishing_web:/app/public/data
      - /data/publishing_crono_log:/app/log
      - /data/publishing_web_private:/app/data
  pub_worker:
    <<: *default-worker
    container_name: pub_worker
    command: bundle exec rails r "Publishing.work('harvest')"
    volumes: # NOTE: YES, it shares the data volumes with app, so it can serve the files that were processed here.
      - /data/publishing_web:/app/public/data
      - /data/publishing_worker_log:/app/log
      - /data/publishing_web_private:/app/data
  pub_data_worker:
    <<: *default-worker
    container_name: pub_data_worker
    command: bundle exec rails r "Publishing.work('download')"
    volumes: # NOTE: YES, it shares the data volumes with app, so it can serve the files that were processed here.
      - /data/publishing_web:/app/public/data
      - /data/publishing_data_log:/app/log
      - /data/publishing_web_private:/app/data
  pub_integrity_worker:
    <<: *default-worker
    container_name: pub_integrity_worker
    command: bundle exec rails r "Publishing.work('data_integrity')"
    volumes: # NOTE: YES, it shares the data volumes with app, so it can serve the files that were processed here.
      - /data/publishing_web:/app/public/data
      - /data/publishing_integrity_log:/app/log
      - /data/publishing_web_private:/app/data
  nginx:
    <<: *default-deploy
    profiles: ["load"]
    build:
      context: ..
      dockerfile: ./nginx.Dockerfile
    container_name: nginx
    volumes:
      - ../config/nginx.conf:/etc/nginx/nginx.conf:ro
      - /data/publishing_nginx_log:/var/log/nginx
      - /data/publishing_web:/app/public/data:ro
    ports:
      - "80:80"
    depends_on:
      - app